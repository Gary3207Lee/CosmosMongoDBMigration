# Monitoring Guide for Online Migration with Databricks

## Monitor Progress

As the task begins its execution, navigate to the Spark User Interface (UI) within Azure Databricks. Observe that the task triggers multiple jobs, often exceeding one per cluster used.

After a few minutes of task execution, you will notice the commencement of data transfer to the destination database. As the migration job progresses, various avenues provide insights into the job's status. These observations help in ensuring the smooth execution of the task. Refer to the following methods to assess the job's performance and address any issues:

### Databricks UI Monitoring

Access the Databricks UI to gain insights into the job's tasks and their progress. 
Spark UI - Jobs: Monitor different job states, including:

- Active Jobs 
- Completed Jobs 
- Failed Jobs (most pertinent) - Investigate failed jobs by examining the trace. </br>

Spark UI - Executors: For each executor (node), review the following details: 
- Memory Usage 
- Active, Failed, and Completed Tasks 
- Task Time and GC Time - A high GC time with a red executor indicates potential issues. </br>

Spark UI - Logs: For failed tasks, inspect error logs to identify any problems.

- Executor State and Loss Reason: Examine executor state, loss reason, and code to understand failures.
- Source Database Throttling: Look for exceptions in the logs that might indicate source database throttling requests. </br>

### Status Server Logs

Migration tool logs are written to the status server (Cosmos NoSQL DB) specified in the config file.
Focus on specific collections within the status server for debugging:

- _status collection: Tracks migration status for each collection, including the number of migrated records.
- _error collection: Logs entries for documents that the job couldn't insert initially. </br>

### Cosmos DB Metrics

Use Cosmos DB metrics to assess the writing process and identify potential issues, by diligently monitoring these aspects, you can effectively gauge the progress of your migration job and swiftly address any issues that may arise.

Total Requests by Response Codes:
Analyze the distribution of response codes (e.g., 201, 429, 413).
201: Successful request.
429: Increase RU/sec, preferably doubling it.
403: Limit reached for unsharded collection of 20GB.
Normalized RU Utilization by Partition:
Monitor RU utilization across partitions to ensure even distribution.
If RU utilization isn't uniform, it might indicate partition skew.
Possible causes include a Shard Keywith low cardinality or non-incremental RU/sec values.
Status Codes Analysis:
Pay attention to the status codes accompanying various requests, such as 201, 429, and 413, among others. These codes provide insights into the response of the destination database during migration. </br>

</br>

## How to read status server

Assuming the entry in config looks like this:
  <pre>
  <code>
  ...
  "status": {
    "serverName": "statusServer",
    "database": "database_name",
    "collection": "migration_status",
    "throughput": 4000
    },
  ...
  </code>
  </pre>

1. Goto the portal UI of the Cosmos DB Nosql Server for statusServer
2. In the data explorer find the databae named database_status
3. Within the db database_status look for the collection migration_status
4. Now run the query: select * from c where c.collection = “target_collection_name”
5. The result will look like this:
  <pre>
  <code>
  ...
  {  

    "id": "38804944-2c30-486c-85b6-76309e7df9d8",  
    "migrationType": "BULK_COPY",  
    "parentId": null,  
    "sourceName": "sourceDBServer",  
    "database": "sourceDBName",  
    "collection": "sourceCollection",  
    "sessionId": "e0e1cc6d-1c47-4705-be5a-533d89a2df6d",  
    "timestamp": "Wed Aug 02 17:22:23 UTC 2023",  
    "streamingToIntermediateCheckpoint": "/tmp/streaming/intermediate/1690996943511/ sourceDBName / sourceCollection /",  
    "streamingToDestinationCheckpoint": "/tmp/streaming/destination/1690996943511/ destinationDBName / destinationCollection /",  
    "sourceDocumentCount": 825583,  
    "destinationDocumentCountAfterBulkCopy": 825583,  
    "destinationDocumentCountAtEnd": 825583,  
    "finalDocumentCount": 825583,  
    "status": "SUCCESSFUL",  
    "_rid": "cAEQAPZM2WUCAAAAAAAAAA==",  
    "_self": "dbs/cAEQAA==/colls/cAEQAPZM2WU=/docs/cAEQAPZM2WUCAAAAAAAAAA==/",  
    "_etag": "\"0200fdc0-0000-0700-0000-64caa2c40000\"",  
    "_attachments": "attachments/",  
    "_ts": 1691001540  
  }  
  ...
  </code>
  </pre>

The keys which are relevant are: collection, sourceDocumentCount, … finalDocumentCount, status.  
If the status is successful and the finalDocumentCount matches sourceDocumentCount, then the job is successful.  
If the result looks like this:
  <pre>
  <code>
  ...
  {  
     "id": "af65102d-d073-44c4-b801-536aa6fdc2a0",  
     "migrationType": "BULK_COPY",  
     "parentId": null,  
     "sourceName": "sourceDBServer",  
     "database": "sourceDBName",  
     "collection": "sourceCollection",  
     "sessionId": "e0e1cc6d-1c47-4705-be5a-533d89a2df6d",  
     "timestamp": "Wed Aug 02 17:22:48 UTC 2023",  
     "streamingToIntermediateCheckpoint": "/tmp/streaming/intermediate/1690996943511/ sourceDBName / sourceCollection /",  
     "streamingToDestinationCheckpoint": "/tmp/streaming/destination/1690996943511/ destinationDBName / destinationCollection /",  
     "sourceDocumentCount": 874,  
     "destinationDocumentCountAfterBulkCopy": 862,  
     "destinationDocumentCountAtEnd": -1,  
     "finalDocumentCount": 0,  
     "status": "PARTIAL",  
     "_rid": "cAEQAPZM2WUFAAAAAAAAAA==",  
     "_self": "dbs/cAEQAA==/colls/cAEQAPZM2WU=/docs/cAEQAPZM2WUFAAAAAAAAAA==/",  
     "_etag": "\"0200724d-0000-0700-0000-64ca91ea0000\"",  
     "_attachments": "attachments/",  
     "_ts": 1690997226  
  }  
  ...
  </code>
  </pre>

In this case the status is PARTIAL and the document count after bulk copy is less than that in the source, it means that the job was not successful. So we need to look in the collection: migration_status_error for the reasons behind these missing records, so we run this query on this collection:
  <pre>
  <code>
  ...
  SELECT * FROM c where c.collection = ‘sourceCollection’
  ...
  </code>
  </pre>

In the resulting records, look for the key “error”, one such example is:
  <pre>
  <code>
  ...
  "error": "com.mongodb.MongoWriteException: Write operation error on server agys-stay-poc-eastus.mongo.cosmos.azure.com:10255. Write error: WriteError{code=16, message='Error=16, Details='Response status code does not indicate success: RequestEntityTooLarge (413); Substatus: 0; ActivityId: 94982c55-81c8-4067-a459-3736049a3280; Reason: (Message: {\"Errors\":[\"Request size is too large\"]}\r\nActivityId: 94982c55-81c8-4067-a459-3736049a3280, Request URI: /apps/248685e1-d438-403e-a3dc-6993b5ab9c6b/services/e06197e9-51eb-4cc0-a20e-fde549009559/partitions/3d408a23-3090-4d19-ba01-f50bf758c738/replicas/133274793010762545p/, RequestStats: Microsoft.Azure.Cosmos.Tracing.TraceData.ClientSideRequestStatisticsTraceDatum, SDK: Windows/10.0.17763 cosmos-netstandard-sdk/3.18.0);', details={}}."
  ...
  </code>
  </pre>
In this case the request size is too large so it looks like the size of the document is larger than the max limit at Cosmos DB which is 2 MB extendable to 16 MB.