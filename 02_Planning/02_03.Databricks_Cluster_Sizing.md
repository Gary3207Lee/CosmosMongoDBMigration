# Databricks_Cluster_Sizing

The size of your Databricks cluster plays a pivotal role in supporting read and write throughput. Consider the following guidelines when determining cluster size: 
- Low Read/Write Throughput: For scenarios with low read or write throughput, a single-node cluster with specifications such as 32/64 cores and 128/256 GB memory is suitable. 
- High Read/Write Throughput: When dealing with high read and write throughput requirements, opt for a multi-node cluster composed of 2 to 4 nodes. This configuration ensures the necessary processing power for efficient data migration. 

## Performance Experiments (RU based Mongo DB)

We conducted performance experiments using the following setup: 
- Source: 79,367,900 documents, approximate size 1 KB, Storage Size 48.85 GB 
- Target: 79,367,900 documents, approximate size 1 KB, Storage 231.7 GB 

Migration was executed with varying RUs provisioned for Cosmos DB. The Databricks cluster remained consistent: 
- Driver: Standard_D16s_v5 · Workers: Standard_D16s_v5 
- 8 workers · 12.1 (includes Apache Spark 3.3.1, Scala 2.12) 
- Each worker and driver: 64GB RAM, 16 cores 

Results based on RUs provisioned:
|RUs provisioned|Cluster Size|Time taken|Notes|
|---------------|------------|----------|-----|
|100,000|16 GB, 8 * 16 cores|8 hours|  |
|200,000|16 GB, 8 * 16 cores|5 hours|  |
|400,000|16 GB, 8 * 16 cores|2.8 hours|  |

Another experiment involved MongoDB with 2.28 billion documents (approximately 1 KB each), totalling 1.4 TB storage. Results: 

|RUs provisioned|Cluster Size|Time taken|Notes|
|---------------|------------|----------|-----|
|1,000,000|16GB, 12 * 16 cores|43.9 hours|  |
|800,000|16GB, 24 * 16 cores|27.7 hours|  |
|800,000|16GB, 24 * 16 cores|9 hours|Removed wildcard ($**) index.|

From our experiments, we observed that Spark can efficiently utilize a maximum of 2500 RUs of Cosmos for MongoDB API per core per executor of a Databricks job. 