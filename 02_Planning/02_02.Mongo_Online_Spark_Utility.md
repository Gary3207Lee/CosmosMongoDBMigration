# MongoDB to Azure Cosmos DB for Mongo DB Online Migration - Spark Utility User Guide

</br>

## Introduction

This guide provides a detailed walkthrough of the data migration process from MongoDB (native) to Azure Cosmos DB for MongoDB (RU/vCore) using Azure Databricks and Spark. </br>

**Important Note**: </br>
The Azure Cosmos DB Migration for MongoDB extension within Azure Data Studio plays a crucial role in evaluating MongoDB workloads for migration to Azure Cosmos DB for MongoDB. This extension empowers you to perform a comprehensive assessment of your MongoDB workload, assisting in the identification of essential steps for a seamless transition to Azure Cosmos DB. By utilizing this extension, you can do a holistic evaluation of your MongoDB workload, gaining actionable insights that contribute to a successful migration onto the Azure Cosmos DB platform. It is strongly recommended to assess your MongoDB workload before initiating the migration process. Refer to [Azure Cosmos DB Migration for MongoDB extension - Azure Data Studio | Microsoft Learn](https://learn.microsoft.com/en-us/azure-data-studio/extensions/database-migration-for-mongo-extension?view=sql-server-ver16) for steps to assess your MongoDB workload. </br>

</br>

Migrations can be done in 2 ways: </br>

- **Offline Migration**: </br>
A snapshot based bulk copy from source to target. New data added/updated/deleted on the source after the snapshot will not be copied to the target. The application downtime required will depend on the time taken for the bulk copy activity to complete. 

- **Online Migration**: </br>
Apart from the bulk data copy activity done in the offline migration, a change stream monitors all additions/updates/deletes and stores them into an intermediate data store. After the bulk data copy is completed the data in the intermediate is copied to the target to make sure all updates done during the migration process are also copied to the target. The application downtime required will be minimal.

</br>

![02_01.MIG_Journey.png](/02_Planning/Image/02_01.MIG_Journey.png)

The migration process involves the following phases: </br>

- **Pre-migration**: </br>
This step involves discovery and assessing readiness of your resources to for migration. Assessment involves finding out whether you're using the features and syntax that are supported. It also includes making sure you're adhering to the limits and quotas. The aim of this stage is to create a list of incompatibilities and warnings, if any. Refer to [Azure Cosmos DB Migration for MongoDB extension - Azure Data Studio | Microsoft Learn](https://learn.microsoft.com/en-us/azure-data-studio/extensions/database-migration-for-mongo-extension?view=sql-server-ver16) for steps to assess your MongoDB workload.

- **Migration**: </br> 
This step is where you configure the migration environment, Spark Migration Utility and start executing the migration.  

- **Monitor Progress**: </br> 
Migration can be long running process and can take several hours to complete, in this this phase you monitor the various metrics and makes sure the migration is running as per expectation.  

- **Cut Over**: </br>
This phase is only applicable to online migrations and is used to cut over the incoming traffic from the current source to the new target. 

- **Post Migration**: </br> 
This step involves post migration steps, like optimizing the index policies, configuring your Azure Cosmos DB Mongo account for global distribution, etc. </br>

</br>

## Spark Migration Utility Overview

![02_02.Overview.png](/02_Planning/Image/02_02.Overview.png)

The Spark Migration Utility is running as a job in Databricks. The state of each migration is persisted in a Azure Cosmos NOSQL account which must be provisioned upfront. This Azure Cosmos DB NoSQL account contains collections which persists migration status, any errors as well as any documents which could not be migrated. </br>

## Prerequisites

### Connectivity

The Databricks cluster must be able to connect to the source Mongo DB, the Azure Cosmos NoSQL account  and the Azure Cosmos DB Mongo account. </br>

Recommended is to deploy Databricks in a custom VNET which is either peered through VPN or ExpressRoute with your Mongo DB instance. In cases where the source is not within a VPN environment, to ensure optimal data transfer and access speed, consider locating your Databricks cluster within the same region as the source data. </br>

When dealing with a Virtual Private Network (VPN) environment, consider the following options: </br>
  - **Place Azure Databricks Cluster within the Same VPN (Recommended)**: </br>
  For optimal performance and security, it is advisable to deploy your Azure Databricks (ADB) cluster within the same VPN as the source data. This approach ensures direct and secure communication between the cluster and the data source.  
  - **VPN Peering with Source VPN**: </br>
  An alternative is to position your Databricks cluster within a VPN and then establish a VPN peering connection with the source VPN. This arrangement eliminates the need for IP address whitelisting and guarantees secure communication.  
  - **Utilize NAT Gateway**: </br>
  While not recommended due to increased latency, you can place your Databricks cluster within a VPN and utilize a Network Address Translation (NAT) Gateway. This gateway ensures that all requests from cluster nodes carry defined IP addresses, which can then be whitelisted for access. </br>

### Connection Strings  

The Spark Migration Utility uses connection strings to connect the following resources.
  - Source Mongo DB Cluster/endpoint
  - Target Cosmos DB MongoDB account
  - Cosmos DB NoSQL account

Verify the connection strings by using appropriate utilities to test connections upfront. </br>

### Migration configuration

The Spark Migration Utility uses a JSON based configuration file as input. A detailed explanation of each option and sample JSON configuration files can be found [here](/99_Practice/First_MIG.json) </br>

</br>

## Tutorial: Configuring Spark Migration Utility

### Cosmos NoSQL Intermediate Configuration

1. Log in to the [Azure Portal](portal.azure.com)
2. Click "Create a resource."
3. Type "cosmosdb" in the search box, locate "Azure Cosmos DB" and click "Create."
  - ![02_03.cosmos_nosql_01](/02_Planning/Image/02_03.cosmos_nosql_01.png)
4. Choose NoSQL
  - ![02_03.cosmos_nosql_02](/02_Planning/Image/02_03.cosmos_nosql_02.png)
5. Select correct Subscription, Resource Group, Account Name and Location. You can leave default the rest.
6. Choose Connectivity Method for your network environment on Networking Tab.
7. Review + Create </br>

</br>

You can refer to the tutorial: [Quickstart: Create an Azure Cosmos DB account, database, container, and items from the Azure portal](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-portal#create-account) </br>

</br>

### Databricks Configuration

#### Create Azure Databricks cluster

1. Log in to the [Azure Portal](portal.azure.com)
2. Click "Create a resource."
3. Type "databricks" in the search box, locate "Azure Databricks" and click "Create."
  - ![02_04.databricks_01.png](/02_Planning/Image/02_04.databricks_01.png)
4. Choose your subscription and resource group. If the resource group doesn't exist, create one for this job. 
5. Provide a name for the Databricks workspace. 
6. Opt for the same region as your MongoDB database (if hosted on Azure) or your Cosmos for MongoDB account.
  -   ![02_04.databricks_02.png](/02_Planning/Image/02_04.databricks_02.png)
7. If you desire a specific VNet for the job Click on "Networking." Select "Yes" for "Deploy Azure Databricks workspace in your own Virtual Network (VNet)."
  - Choose the VNet's name.
  - Specify a name for the public subnet.
  - Determine an available IP address range from your VNet's address space.
  - Assign an IP address range to both the public and private subnets. For example, if your VNet's address space is 10.7.0.0/16, you could use 10.7.101.0/24 for the public subnet and 10.7.100.0/24 for the private subnet.
  -   ![02_04.databricks_03.png](/02_Planning/Image/02_04.databricks_03.png)
8. Click "Review + create."
9. Click "Create."
10. Once created, click "Go to resource." </br>

</br>

#### Configure Azure Databricks for migration

1. Open the Azure Databricks resource created in the above step.
2. Click "Launch Workspace."
  - ![02_05.config_01.png](/02_Planning/Image/02_05.config_01.png)
3. Click "Workflows." From the left blade
4. Click "Create Job."
  - ![02_05.config_02.png](/02_Planning/Image/02_05.config_02.png)
5. In the dialog, complete the following:
  - Assign a name to the task.
  - Select "JAR" for Type.
  - For the main class: com.microsoft.azure.cosmosdb.migration.MongoToCosmosMigration
  - ![02_05.config_03.png](/02_Planning/Image/02_05.config_03.png)
6. Upload the utility jar from the Git repository (latest release version).
  - ![02_05.config_04.png](/02_Planning/Image/02_05.config_04.png)
7. Add the following dependent jars using Maven
  - org.mongodb.spark:mongo-spark-connector_2.12:10.1.1
  - com.azure.cosmos.spark:azure-cosmos-spark_3-3_2-12:4.17.2
  - com.azure:azure-cosmos:4.41.0
  - ![02_05.config_05.png](/02_Planning/Image/02_05.config_05.png)
8. You should see a total of 4 JAR files after upload.
  - ![02_05.config_06.png](/02_Planning/Image/02_05.config_06.png)
9. Create the Job by clicking "Create."
10. Provide a name and save the job.
11. We create a file with the name fairscheduler.xml with following content in it.

  ```
<?xml version="1.0"?> 
<allocations> 
  <pool name="fair"> 
    <schedulingMode>FAIR</schedulingMode>
    <weight>1</weight>
    <minShare>2</minShare>
  </pool>
  <pool name="test">
    <schedulingMode>FIFO</schedulingMode>
    <weight>2</weight>
    <minShare>3</minShare>
  </pool>
</allocations>
  ```

12. We need to upload fairscheduler.xml(you can set your own file name) to databricks data section similar to the way we uploaded the json file.
13. After the job is created, click on the job name in the Workflows screen.
14. Click on "Configure" in the "Compute" section on the right-hand side pane.
  - ![02_05.config_07.png](/02_Planning/Image/02_05.config_07.png)
15. Scroll down to the bottom in the cluster config dialog.
16. In Advanced options, add the following Spark configuration options(edit the values with <..> before pasting)  and then click "Confirm."

```spark.driver.extraJavaOptions -XX:+UseG1GC -XX:G1HeapRegionSize=16M -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -XX:+PrintAdaptiveSizePolicy -XX:AdaptiveSizePolicyOutputInterval=1 -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:MaxDirectMemorySize=4096M -XX:-ResizePLAB -XX:+UseCompressedOops -XX:InitiatingHeapOccupancyPercent=45 -XX:+UnlockExperimentalVMOptions -XX:G1MixedGCLiveThresholdPercent=85 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=4
spark.databricks.delta.preview.enabled true
spark.scheduler.allocation.file file:///dbfs//FileStore/tables/fairscheduler.xml
spark.executor.cores 16
spark.executor.memory 32g
spark.executor.memoryOverheadFactor .1
spark.executor.instances 115
spark.task.maxFailures 1
spark.driver.memory 16g
spark.executor.extraJavaOptions, -XX:-UseParallelGC -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=25 -XX:MaxGCPauseMillis=500 -XX:+ExplicitGCInvokesConcurrent
spark.driver.memoryOverheadFactor .1
spark.databricks.libraries.enableMavenResolution false
spark.driver.cores 7
```

  - ![02_05.config_08.png](/02_Planning/Image/02_05.config_08.png)

</br>

### JSON Configuration

The Spark Migration Utility uses a JSON configuration file as input. See prerequisites for a detailed explanation. You can use this sample JSON. </br>

```{
    "version": 2,
    "servers": {
        "sourceServer": {
            "type": "MONGODB",
            "name": "sourceServer",
            "url": "mongodb://#####"
        },
        "destinationServer": {
            "type": "MONGODB",
            "name": "destinationServer",
            "url": "mongodb://#####"
        },
        "intermediateServer": {
            "type": "COSMOSDB",
            "name": "intermediateServer",
            "url": "https://#####.documents.azure.com:443/",
            "key": "#####"
        },
        "statusServer": {
            "type": "COSMOSDB",
            "name": "statusServer",
            "url": "https://#####.documents.azure.com:443/",
            "key": "#####"
        }
    },
    "tasks": [
        {
            "migrationUnit": "COLLECTION",
            "source": {
                "serverName": "sourceServer",
                "databases": [
                    "MIGTestDB"
                ],
                "collections": [
                    "MIGTestCollection"
                ]
            },
            "destination": {
                "serverName": "destinationServer",
                "databases": [
                    "MIGTestDB"
                ],
                "collections": [
                    "MIGTestCollection"
                ],
                "throughput": 20000,
                "migrationThroughput": 20000,
                "scaling": "AUTOSCALE",
                "shardKey": "itemcode"
            },
            "intermediate": {
                "serverName": "intermediateServer",
                "databases": [
                    "IntermediateDB"
                ],
                "collections": [
                    "IntermediateCollection"
                ],
                "throughput": 20000
            },
            "excludedCollections": [
                "system.views"
            ],
            "uniqueConstraintViolationHandling":"IGNORE",
            "indexCreation":"ALL_INDEXES",
            "batchCount":30
        }
    ],
    "status": {
        "serverName": "statusServer",
        "database": "StatusDB",
        "collection": "StatusCollection",
        "throughput": 1000
    },
    "copyPartitions": 100,
    "copyBatchSize": 30,
    "streamingPartitions": 100,
    "streamingBatchSize": 30,
    "startStreamToIntermediate": false,
    "startBulkCopy": true,
    "startStreamToDestination": false,
    "offHeapMemory": false
}
```

01. Go to Catalog and Click Add
  - ![02_06.JSON_01.png](/02_Planning/Image/02_06.JSON_01.png)
02. Select DBFS
  - ![02_06.JSON_02.png](/02_Planning/Image/02_06.JSON_02.png)
03. Drag & Drop your JSON file
  - ![02_06.JSON_03.png](/02_Planning/Image/02_06.JSON_03.png)
04. Go back to your JOB and select Tasks menu.
  - ![02_06.JSON_04.png](/02_Planning/Image/02_06.JSON_04.png)
05. Put your uploaded JSON file path like below
  - ![02_06.JSON_05.png](/02_Planning/Image/02_06.JSON_05.png)

```["-Dmigrator.parametersJsonFile=/FileStore/tables/mig_param/First_MIG.json"]```

</br>

## Run Migration Job

Click "Run Now" </br>
![02_07.RUN.png](/02_Planning/Image/02_07.RUN.png) </br>

</br>

## Complete the migration

As the task begins its execution, navigate to the Spark (UI) within Azure Databricks. The first step of the execution is partitioning the source collection documents in Spark. This allows Spark to migration documents more efficiently. Be aware that it might take several minutes or longer before the partitioning is finished, and the migration starts. When you click on the running Job and select Spark UI observe that the task triggers one or more bulk copy jobs.

Once the migration is completed your Job status will show Succeeded. You can also validate this by examining the status in the Cosmos NoSQL account: </br>

1. Navigate to the Cosmos NoSQL account
2. Click on Data Explorer and select the migration database you have specified in the JSON configuration file
3. Click on migration status container and click on new SQL query
4. Run: SELECT * FROM c where c.database = 'yourdbname'  
5. In the JSON retrieved you can examine the status.

![02_08.Completion.png](/02_Planning/Image/02_08.Completion.png) </br>

</br>

## Post-migration optimization

After you migrate the data stored in MongoDB database to Azure Cosmos DB for MongoDB, you can connect to Azure Cosmos DB and manage the data. You can also perform other post-migration optimization steps such as optimizing the indexing policy, update the default consistency level, or configure global distribution for your Azure Cosmos DB account. </br>

</br>

## Next Steps

- Monitoring and troubleshooting
- Review migration guidance for online migration
- Review sample JSON files for different scenarios
