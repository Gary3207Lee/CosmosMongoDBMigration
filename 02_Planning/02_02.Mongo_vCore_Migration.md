# MongoDB to Azure Cosmos DB for Mongo DB- Spark Utility User Guide

</br>

## Introduction

This guide provides a detailed walkthrough of the data migration process from MongoDB (native) to Azure Cosmos DB for MongoDB (RU/vCore) using Azure Databricks and Spark. </br>

Important Note: The Azure Cosmos DB Migration for MongoDB extension within Azure Data Studio plays a crucial role in evaluating MongoDB workloads for migration to Azure Cosmos DB for MongoDB. This extension empowers you to perform a comprehensive assessment of your MongoDB workload, assisting in the identification of essential steps for a seamless transition to Azure Cosmos DB. By utilizing this extension, you can do a holistic evaluation of your MongoDB workload, gaining actionable insights that contribute to a successful migration onto the Azure Cosmos DB platform. It is strongly recommended to assess your MongoDB workload before initiating the migration process. Refer to [Azure Cosmos DB Migration for MongoDB extension - Azure Data Studio | Microsoft Learn](https://learn.microsoft.com/en-us/azure-data-studio/extensions/database-migration-for-mongo-extension?view=sql-server-ver16) for steps to assess your MongoDB workload. </br>

</br>

Migrations can be done in 2 ways: </br>

- Offline Migration: </br>
A snapshot based bulk copy from source to target. New data added/updated/deleted on the source after the snapshot will not be copied to the target. The application downtime required will depend on the time taken for the bulk copy activity to complete. 

- Online Migration: </br>
Apart from the bulk data copy activity done in the offline migration, a change stream monitors all additions/updates/deletes and stores them into an intermediate data store. After the bulk data copy is completed the data in the intermediate is copied to the target to make sure all updates done during the migration process are also copied to the target. The application downtime required will be minimal.

</br>

![02_01.MIG_Journey.png](/02_Planning/Image/02_01.MIG_Journey.png)

The migration process involves the following phases: </br>

- Planning, Estimating, and Environment Setup: </br>
This phase is premigration and involves the estimations and network configuration related activities required to have a seamless migration experience. </br>

- Setting Up Configuration JSON File: </br>
This phase covers the creation of the JSON file that holds migration configurations. The JSON file is essential for the migration process and needs to be uploaded to the Azure Databricks workspace. </br>

- Setting Up Azure Databricks Cluster and Running Migration: </br>
This phase is where you configure the Azure Databricks cluster and start executing the migration. The JSON file created in the previous phase is a critical input here. </br>

- Monitor Progress: </br>
Migration can be long running process and can take several hours to complete, in this this phase you monitor the various metrics and makes sure the migration is running as per expectation. </br>

- Cut Over: </br>
This phase is only applicable to online migrations and is used to cut over the incoming traffic from the current source to the new target. </br>

## Overview

![02_02.Overview.png](/02_Planning/Image/02_02.Overview.png)

<add explanation -> Cyrille> 

## Prerequisites

< add source and target prereqs> 
< add planning> Use a link to another document. Cyrille 

## Cosmos NoSQL Intermediate Configuration

1. Log in to the [Azure Portal](portal.azure.com)
2. Click "Create a resource."
3. Type "cosmosdb" in the search box, locate "Azure Cosmos DB" and click "Create."
  - ![02_03.cosmos_nosql_01](/02_Planning/Image/02_03.cosmos_nosql_01.png)
4. Choose NoSQL
  - ![02_03.cosmos_nosql_02](/02_Planning/Image/02_03.cosmos_nosql_02.png)
5. Select correct Subscription, Resource Group, Account Name and Location. You can leave default the rest.
6. Choose Connectivity Method for your network environment on Networking Tab.
7. Review + Create </br>

</br>

You can refer to the tutorial: [Quickstart: Create an Azure Cosmos DB account, database, container, and items from the Azure portal](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-portal#create-account) </br>

</br>

## Databricks Configuration

### Create Azure Databricks cluster.

1. Log in to the [Azure Portal](portal.azure.com)
2. Click "Create a resource."
3. Type "databricks" in the search box, locate "Azure Databricks" and click "Create."
  - ![02_04.databricks_01.png](/02_Planning/Image/02_04.databricks_01.png)
4. Choose your subscription and resource group. If the resource group doesn't exist, create one for this job. 
5. Provide a name for the Databricks workspace. 
6. Opt for the same region as your MongoDB database (if hosted on Azure) or your Cosmos for MongoDB account.
  -   ![02_04.databricks_02.png](/02_Planning/Image/02_04.databricks_02.png)
7. If you desire a specific VNet for the job Click on "Networking." Select "Yes" for "Deploy Azure Databricks workspace in your own Virtual Network (VNet)."
  - Choose the VNet's name.
  - Specify a name for the public subnet.
  - Determine an available IP address range from your VNet's address space.
  - Assign an IP address range to both the public and private subnets. For example, if your VNet's address space is 10.7.0.0/16, you could use 10.7.101.0/24 for the public subnet and 10.7.100.0/24 for the private subnet.
  -   ![02_04.databricks_03.png](/02_Planning/Image/02_04.databricks_03.png)
8. Click "Review + create."
9. Click "Create."
10. Once created, click "Go to resource." </br>

</br>

### Configure Azure Databricks for migration

1. Open the Azure Databricks resource created in the above step.
2. Click "Launch Workspace."
  - ![02_05.config_01.png](/02_Planning/Image/02_05.config_01.png)
3. Click "Workflows." From the left blade
4. Click "Create Job."
  - ![02_05.config_02.png](/02_Planning/Image/02_05.config_02.png)
5. In the dialog, complete the following:
  - Assign a name to the task.
  - Select "JAR" for Type.
  - For the main class: com.microsoft.azure.cosmosdb.migration.MongoToCosmosMigration
  - ![02_05.config_03.png](/02_Planning/Image/02_05.config_03.png)
6. Upload the utility jar from the Git repository (latest release version).
  - ![02_05.config_04.png](/02_Planning/Image/02_05.config_04.png)
7. Add the following dependent jars using Maven
  - org.mongodb.spark:mongo-spark-connector_2.12:10.1.1
  - com.azure.cosmos.spark:azure-cosmos-spark_3-3_2-12:4.17.2
  - com.azure:azure-cosmos:4.41.0
  - ![02_05.config_05.png](/02_Planning/Image/02_05.config_05.png)
8. You should see a total of 4 JAR files after upload.
  - ![02_05.config_06.png](/02_Planning/Image/02_05.config_06.png)
9. Create the Job by clicking "Create."
10. Provide a name and save the job.
11. We create a file with the name fairscheduler.xml with following content in it.

  ```
<?xml version="1.0"?> 
<allocations> 
  <pool name="fair"> 
    <schedulingMode>FAIR</schedulingMode>
    <weight>1</weight>
    <minShare>2</minShare>
  </pool>
  <pool name="test">
    <schedulingMode>FIFO</schedulingMode>
    <weight>2</weight>
    <minShare>3</minShare>
  </pool>
</allocations>
  ```

12. We need to upload faischeduler.xml to databricks data section similar to the way we uploaded the json file.
13. After the job is created, click on the job name in the Workflows screen.
14. Click on "Configure" in the "Compute" section on the right-hand side pane.
  - ![02_05.config_07.png](/02_Planning/Image/02_05.config_07.png)
15. Scroll down to the bottom in the cluster config dialog.
16. In Advanced options, add the following Spark configuration options(edit the values with <..> before pasting)  and then click "Confirm."
  - ![02_05.config_08.png](/02_Planning/Image/02_05.config_08.png)

```spark.driver.extraJavaOptions -XX:+UseG1GC -XX:G1HeapRegionSize=16M -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -XX:+PrintAdaptiveSizePolicy -XX:AdaptiveSizePolicyOutputInterval=1 -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:MaxDirectMemorySize=4096M -XX:-ResizePLAB -XX:+UseCompressedOops -XX:InitiatingHeapOccupancyPercent=45 -XX:+UnlockExperimentalVMOptions -XX:G1MixedGCLiveThresholdPercent=85 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=4
spark.databricks.delta.preview.enabled true
spark.scheduler.allocation.file file:///dbfs//FileStore/tables/fairscheduler.xml
spark.executor.cores 16
spark.executor.memory 32g
spark.executor.memoryOverheadFactor .1
spark.executor.instances 115
spark.task.maxFailures 1
spark.driver.memory 16g
spark.executor.extraJavaOptions, -XX:-UseParallelGC -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=25 -XX:MaxGCPauseMillis=500 -XX:+ExplicitGCInvokesConcurrent
spark.driver.memoryOverheadFactor .1
spark.databricks.libraries.enableMavenResolution false
spark.driver.cores 7
```

</br>

## JSON Configuration

The migration tool configuration setting is supplied via a JSON file. This JSON contains three main blocks. The details of each block and its parameters details are provided below.

```{
    "version": 2,
    "servers": {
        "sourceServer": {
            "type": "MONGODB",
            "name": "sourceServer",
            "url": "mongodb://#####"
        },
        "destinationServer": {
            "type": "MONGODB",
            "name": "destinationServer",
            "url": "mongodb://#####"
        },
        "intermediateServer": {
            "type": "COSMOSDB",
            "name": "intermediateServer",
            "url": "https://#####.documents.azure.com:443/",
            "key": "#####"
        },
        "statusServer": {
            "type": "COSMOSDB",
            "name": "statusServer",
            "url": "https://#####.documents.azure.com:443/",
            "key": "#####"
        }
    },
    "tasks": [
        {
            "migrationUnit": "COLLECTION",
            "source": {
                "serverName": "sourceServer",
                "databases": [
                    "MIGTestDB"
                ],
                "collections": [
                    "MIGTestCollection"
                ]
            },
            "destination": {
                "serverName": "destinationServer",
                "databases": [
                    "MIGTestDB"
                ],
                "collections": [
                    "MIGTestCollection"
                ],
                "throughput": 20000,
                "migrationThroughput": 20000,
                "scaling": "AUTOSCALE",
                "shardKey": "itemcode"
            },
            "intermediate": {
                "serverName": "intermediateServer",
                "databases": [
                    "IntermediateDB"
                ],
                "collections": [
                    "IntermediateCollection"
                ],
                "throughput": 20000
            },
            "excludedCollections": [
                "system.views"
            ],
            "uniqueConstraintViolationHandling":"IGNORE",
            "indexCreation":"ALL_INDEXES",
            "batchCount":30
        }
    ],
    "status": {
        "serverName": "statusServer",
        "database": "StatusDB",
        "collection": "StatusCollection",
        "throughput": 1000
    },
    "copyPartitions": 100,
    "copyBatchSize": 30,
    "streamingPartitions": 100,
    "streamingBatchSize": 30,
    "startStreamToIntermediate": false,
    "startBulkCopy": true,
    "startStreamToDestination": false,
    "offHeapMemory": false
}
```
01. Go to Catalog and Click Add
  - ![02_06.JSON_01.png](/02_Planning/Image/02_06.JSON_01.png)
02. Select DBFS
  - ![02_06.JSON_02.png](/02_Planning/Image/02_06.JSON_02.png)
03. Drag & Drop your JSON file
  - ![02_06.JSON_03.png](/02_Planning/Image/02_06.JSON_03.png)
04. Go back to your JOB and select Tasks menu.
  - ![02_06.JSON_04.png](/02_Planning/Image/02_06.JSON_04.png)
05. Put your uploaded JSON file path like below
  - ![02_06.JSON_05.png](/02_Planning/Image/02_06.JSON_05.png)

```["-Dmigrator.parametersJsonFile=/FileStore/tables/mig_param/First_MIG.json"]```

</br>

## Run Migration Job

Click "Run Now" </br>
![02_07.RUN.png](/02_Planning/Image/02_07.RUN.png) </br>

</br>

## Complete the migration

As the task begins its execution, navigate to the Spark (UI) within Azure Databricks. The first step of the execution is partitioning the source collection documents in Spark. This allows Spark to migration documents more efficiently. Be aware that it might take several minutes or longer before the partitioning is finished and the migration actually starts. Observe that the task triggers multiple jobs, one per partition.

< add some text here and a screen which shows in progress and a screen which shows finished.> </br>

</br>

## Post-migration optimization

After you migrate the data stored in MongoDB database to Azure Cosmos DB for MongoDB, you can connect to Azure Cosmos DB and manage the data. You can also perform other post-migration optimization steps such as optimizing the indexing policy, update the default consistency level, or configure global distribution for your Azure Cosmos DB account. </br>

</br>

## Next Steps

- Monitoring and troubleshooting
- Review migration guidance for online migration
- Review sample JSON files for different scenarios